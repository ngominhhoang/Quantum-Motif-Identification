{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c884f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from qiskit import QuantumCircuit, Aer\n",
    "from qiskit.circuit import Parameter\n",
    "import numpy\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d6babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_partitioning(G, max_node = 30, cluster_limit = 15):\n",
    "    #max_node = len(G.nodes)\n",
    "    check_nodes = [False for i in range(0, max_node)]\n",
    "    G_remaining = G.copy()\n",
    "    list_new = []\n",
    "    G_new = nx.DiGraph()\n",
    "\n",
    "    while True:\n",
    "        check_all = True\n",
    "        for node in G.nodes:\n",
    "            if not check_nodes[node]:\n",
    "                check_all = False\n",
    "                break\n",
    "            if check_all:\n",
    "                break\n",
    "\n",
    "        max_degree = -1\n",
    "        selected_node = -1\n",
    "        for node in G_remaining:\n",
    "            G_remaining.nodes[node]['score'] = 0\n",
    "            if G_remaining.degree[node] > max_degree:\n",
    "                max_degree = G_remaining.degree[node]\n",
    "                selected_node = node\n",
    "        if selected_node == -1:\n",
    "            break\n",
    "        G_new = nx.DiGraph()\n",
    "        G_new.add_node(selected_node)\n",
    "\n",
    "        check_bfs = [False for i in range(0, max_node)]\n",
    "        for node in G_new.nodes:\n",
    "            check_bfs[node] = True\n",
    "\n",
    "        check_bfs[selected_node] = True\n",
    "        queue_bfs = [selected_node]\n",
    "        disjoint_check = True\n",
    "        while len(queue_bfs) != 0 and len(G_new.edges) <= cluster_limit:\n",
    "            head_node = queue_bfs[0]\n",
    "            total_score = 0\n",
    "            for node in G_remaining.neighbors(head_node):\n",
    "                if not check_bfs[node]:\n",
    "                    G_remaining.nodes[node]['score'] += 1\n",
    "            for node in G_remaining.predecessors(head_node):\n",
    "                if not check_bfs[node]:\n",
    "                    G_remaining.nodes[node]['score'] += 1\n",
    "\n",
    "            disjoint_check = True\n",
    "            for node in G_remaining.nodes:\n",
    "                if check_bfs[node]:\n",
    "                    continue\n",
    "                nums_edge = 0\n",
    "                for node_new in G_new.nodes:\n",
    "                    if G_remaining.has_edge(node, node_new) or G_remaining.has_edge(node_new, node):\n",
    "                        nums_edge += 1\n",
    "                if nums_edge + len(G_new.edges) > cluster_limit:\n",
    "                    #print(G_new.edges, nums_edge, node, cluster_limit)\n",
    "                    G_remaining.nodes[node]['score'] = 0\n",
    "                    check_bfs[node] = True\n",
    "\n",
    "            for node in G_remaining.nodes:\n",
    "                total_score += G_remaining.nodes[node]['score']\n",
    "\n",
    "            if total_score == 0:\n",
    "                break\n",
    "\n",
    "            candidate = []\n",
    "            weight = []\n",
    "            for node in G_remaining.nodes:\n",
    "                if G_remaining.nodes[node]['score'] > 0:\n",
    "                    candidate.append(node)\n",
    "                    weight.append(G_remaining.nodes[node]['score']/total_score)\n",
    "\n",
    "            selected_node = numpy.random.choice(numpy.array(candidate), p=weight)\n",
    "            #print(candidate, selected_node)\n",
    "            new_edges = []\n",
    "            for node_new in G_new.nodes:\n",
    "                if G_remaining.has_edge(selected_node, node_new):\n",
    "                    new_edges.append((selected_node, node_new))\n",
    "                if G_remaining.has_edge(node_new, selected_node):\n",
    "                    new_edges.append((node_new, selected_node))\n",
    "            G_new.add_edges_from(new_edges)\n",
    "\n",
    "            G_remaining.nodes[selected_node]['score'] = 0\n",
    "            check_bfs[selected_node] = True\n",
    "            queue_bfs.append(selected_node)\n",
    "            queue_bfs.pop(0)\n",
    "\n",
    "        list_new.append(G_new)\n",
    "        for node in G_new.nodes:\n",
    "            G_remaining.remove_node(node)\n",
    "    return list_new\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436357eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from qiskit import QuantumCircuit, Aer\n",
    "from qiskit.circuit import Parameter\n",
    "import random\n",
    "\n",
    "def mi_calculation_triangle(G, P, weightG, backend):\n",
    "    appearance = [False for i in range(0, len(G.edges))]\n",
    "    HP = []\n",
    "    obj = {'cost': [], 'penalty': []}\n",
    "    node_set = list(G.nodes())\n",
    "    n = len(node_set)\n",
    "    edge_to_index = {}\n",
    "    idx = 0\n",
    "    for ed in G.edges():\n",
    "        i = ed[0]\n",
    "        j = ed[1]\n",
    "        edge_to_index[(i,j)] = idx\n",
    "        #edge_to_index[(j,i)] = idx\n",
    "        edge_to_index[idx] = (i,j)\n",
    "        #edge_to_index[idx] = (j,i)\n",
    "        idx = idx + 1\n",
    "    \n",
    "    for ed in G.edges:\n",
    "        ed_idx = edge_to_index[(ed[0], ed[1])]\n",
    "        obj['cost'].append([ed_idx])\n",
    "        HP.append(set([ed_idx]))\n",
    "        \n",
    "    node_set = list(G.nodes())\n",
    "    for i in range(0, n):\n",
    "        #print(node_0)\n",
    "        s = []\n",
    "        for j in range(0, n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            for k in range(j+1, n):\n",
    "                if k == i:\n",
    "                    continue\n",
    "                for permu in list(itertools.permutations([0,1,2])):\n",
    "                    mapping = {permu[0]: node_set[i], permu[1]: node_set[j], permu[2]: node_set[k]}\n",
    "                    check = True\n",
    "                    motif = []\n",
    "                    for ed in P.edges():\n",
    "                        if not G.has_edge(mapping[ed[0]], mapping[ed[1]]):\n",
    "                            check = False\n",
    "                            break\n",
    "                        else:\n",
    "                            motif.append(edge_to_index[(mapping[ed[0]], mapping[ed[1]])])\n",
    "                    if check:\n",
    "                        s.append(set(motif))\n",
    "                        for ed_mo in motif:\n",
    "                            appearance[ed_mo] = True\n",
    "                \n",
    "    #print(node_set)\n",
    "    ebunch = []\n",
    "    for ed in G.edges:\n",
    "        idx = edge_to_index[ed]\n",
    "        if not appearance[idx]:\n",
    "            ebunch.append(ed)\n",
    "    G.remove_edges_from(ebunch)\n",
    "    \n",
    "    HP = []\n",
    "    HP_cons2 = []\n",
    "    obj = {'cost': [], 'penalty': [], 'constraint_2': []}\n",
    "    node_set = list(G.nodes())\n",
    "    n = len(node_set)\n",
    "    edge_to_index = {}\n",
    "    idx = 0\n",
    "    for ed in G.edges():\n",
    "        i = ed[0]\n",
    "        j = ed[1]\n",
    "        edge_to_index[(i,j)] = idx\n",
    "        #edge_to_index[(j,i)] = idx\n",
    "        edge_to_index[idx] = (i,j)\n",
    "        #edge_to_index[idx] = (j,i)\n",
    "        idx = idx + 1\n",
    "    \n",
    "    check_cost = []\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            for k in range(0, n):\n",
    "                for permu in list(itertools.permutations([0,1,2])):\n",
    "                    mapping = {permu[0]: node_set[i], permu[1]: node_set[j], permu[2]: node_set[k]}\n",
    "                    check = True\n",
    "                    for ed in P.edges():\n",
    "                        if not G.has_edge(mapping[ed[0]], mapping[ed[1]]):\n",
    "                            check = False\n",
    "                            break\n",
    "                    if not check:\n",
    "                        continue\n",
    "                    #print(mapping)\n",
    "                    check_weight = True\n",
    "                    for ed in P.edges():\n",
    "                        if weightG[(mapping[ed[0]], mapping[ed[1]])] != P[ed[0]][ed[1]]['weight']:\n",
    "                            check_weight = False\n",
    "                            break\n",
    "                    if check and check_weight:\n",
    "                        #print(mapping)\n",
    "                        s = []\n",
    "                        for ed in P.edges():\n",
    "                            s.append(edge_to_index[(mapping[ed[0]], mapping[ed[1]])])\n",
    "                        #print(s)\n",
    "                        if set(s) not in check_cost:\n",
    "                            check_cost.append(set(s))\n",
    "                            obj['cost'].append(s)\n",
    "    for ed in G.edges:\n",
    "        ed_idx = edge_to_index[(ed[0], ed[1])]\n",
    "        #obj['cost'].append([ed_idx])\n",
    "        HP.append(set([ed_idx]))\n",
    "    \n",
    "    node_set = list(G.nodes())\n",
    "    for i in range(0, n):\n",
    "        #print(node_0)\n",
    "        s_cons1 = []\n",
    "        for j in range(0, n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            s_cons2 = []\n",
    "            for k in range(0, n):\n",
    "                if k == i or k == j:\n",
    "                    continue\n",
    "                for permu in list(itertools.permutations([0,1,2])):\n",
    "                    mapping = {permu[0]: node_set[i], permu[1]: node_set[j], permu[2]: node_set[k]}\n",
    "                    check = True\n",
    "                    motif = []\n",
    "                    for ed in P.edges():\n",
    "                        if not G.has_edge(mapping[ed[0]], mapping[ed[1]]):\n",
    "                            check = False\n",
    "                            break\n",
    "                        else:\n",
    "                            #if mapping[ed[0]] != node_set[i] and mapping[ed[1]] != node_set[j]:\n",
    "                            motif.append(edge_to_index[(mapping[ed[0]], mapping[ed[1]])])\n",
    "                    if not check:\n",
    "                        continue\n",
    "                    check_weight = True\n",
    "                    for ed in P.edges():\n",
    "                        if weightG[(mapping[ed[0]], mapping[ed[1]])] != P[ed[0]][ed[1]]['weight']:\n",
    "                            check_weight = False\n",
    "                            break\n",
    "                    if check and check_weight:\n",
    "                        if set(motif) not in s_cons1:\n",
    "                            s_cons1.append(set(motif))\n",
    "                        s_cons2.append(tuple(motif))\n",
    "                        break\n",
    "                \n",
    "                    \n",
    "            if G.has_edge(node_set[i],node_set[j]):\n",
    "                obj['constraint_2'].append((edge_to_index[(node_set[i],node_set[j])], s_cons2))\n",
    "                for idx_0 in range(len(s_cons2)):\n",
    "                    it_0 = s_cons2[idx_0]\n",
    "                    HP_cons2.append([-1, set(it_0 + tuple([edge_to_index[(node_set[i],node_set[j])]]))])\n",
    "                    for idx_1 in range(idx_0 + 1, len(s_cons2)):\n",
    "                        it_1 = s_cons2[idx_1]\n",
    "                        if it_0 != it_1:\n",
    "                            candidate = set(it_0+it_1)\n",
    "                            #if candidate not in HP:\n",
    "                            HP_cons2.append([1, candidate])\n",
    "                \n",
    "        if len(s_cons1) > 1:\n",
    "            ss = []\n",
    "            for ele in s_cons1:\n",
    "                ss.append(tuple(ele))\n",
    "            s_cons1 = ss\n",
    "            obj['penalty'].append(s_cons1)\n",
    "            for idx_0 in range(len(s_cons1)):\n",
    "                for idx_1 in range(idx_0+1, len(s_cons1)):\n",
    "                    it_0 = s_cons1[idx_0]\n",
    "                    it_1 = s_cons1[idx_1]\n",
    "                    if it_0 != it_1:\n",
    "                        candidate = set(it_0+it_1)\n",
    "                        #if candidate not in HP:\n",
    "                        HP.append(candidate)\n",
    "    if len(obj['cost']) == 0:\n",
    "        return 0, [], [], -1\n",
    "    start = time.time()\n",
    "    expectation = get_expectation_test(G, HP, HP_cons2, obj, backend)\n",
    "    res_ = minimize(expectation,\n",
    "                   [0.1, 0.1],\n",
    "                   method='COBYLA')\n",
    "    qc_res = create_qaoa_circ(G, res_.x, HP, HP_cons2)\n",
    "\n",
    "    counts = backend.run(qc_res, seed_simulator=10, shots = 100000).result().get_counts()\n",
    "    minn = 1000000\n",
    "    removed = []\n",
    "    saved = []\n",
    "    for k in counts.keys():\n",
    "        reverse_str = ''\n",
    "        for b in k:\n",
    "            reverse_str = b + reverse_str\n",
    "        maxcut = maxcut_obj(reverse_str, obj)\n",
    "        minn = min(minn, maxcut)\n",
    "        if maxcut % 10 != 0:\n",
    "            continue\n",
    "        if minn == maxcut:\n",
    "            #print(obj)\n",
    "            removed = []\n",
    "            saved = []\n",
    "            for comb in obj['cost']:\n",
    "                summ = 0\n",
    "                for b in comb:\n",
    "                    summ = summ + int(reverse_str[b])\n",
    "                if summ == len(comb):\n",
    "                    new_motif = []\n",
    "                    for b in comb:\n",
    "                        removed.append(edge_to_index[b][0])\n",
    "                        removed.append(edge_to_index[b][1])\n",
    "                        new_motif.append((edge_to_index[b][0], edge_to_index[b][1]))\n",
    "                    saved.append(new_motif)\n",
    "    if minn >= 1000000:\n",
    "        minn = 0\n",
    "    return minn, list(set(removed)), saved, time.time() - start, counts, obj, qc_res, HP, HP_cons2, expectation, edge_to_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb52580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxcut_obj(solution, obj):\n",
    "    res = 0\n",
    "    for comp in obj['cost']:\n",
    "        s = 1\n",
    "        for it in comp:\n",
    "            s = s * int(solution[it])\n",
    "        res = res + 10*s\n",
    "    #print(\"Maxcut:\",comp)\n",
    "    for comp in obj['penalty']:\n",
    "        s = 0\n",
    "        for subcomp in comp:\n",
    "            ss = 1\n",
    "            for it in subcomp:\n",
    "                ss = ss * int(solution[it])\n",
    "            s += ss\n",
    "        w = 49\n",
    "        res = res - w*(1 - s)*(1 - s) - w*s*s + w\n",
    "    for comp in obj['constraint_2']:\n",
    "        s = 0\n",
    "        init_element = comp[0]\n",
    "        for subcomp in comp[1]:\n",
    "            ss = 1\n",
    "            for it in subcomp:\n",
    "                ss = ss * int(solution[it])\n",
    "            s += ss\n",
    "        w = 49\n",
    "        res = res - w*(int(solution[init_element]) - s)*(int(solution[init_element]) - s)\n",
    "    return -res\n",
    "\n",
    "\n",
    "def compute_expectation(counts, graph, obj):\n",
    "    avg = 0\n",
    "    sum_count = 0\n",
    "    for bit_string, count in counts.items():\n",
    "        reverse_str = ''\n",
    "        for b in bit_string:\n",
    "            reverse_str = b + reverse_str\n",
    "        obj_ = maxcut_obj(reverse_str, obj)\n",
    "        avg += obj_ * count\n",
    "        sum_count += count\n",
    "    #print(avg, sum_count)\n",
    "    return avg/sum_count\n",
    "\n",
    "\n",
    "def create_qaoa_circ(graph, theta, HP, HP_cons2):\n",
    "    nqubits = len(graph.edges())\n",
    "    n_layers = len(theta)//2  # number of alternating unitaries\n",
    "    beta = theta[:n_layers]\n",
    "    gamma = theta[n_layers:]\n",
    "\n",
    "    qc = QuantumCircuit(nqubits)\n",
    "\n",
    "    # initial_state\n",
    "    qc.h(range(nqubits))\n",
    "    #print(\"HAHA\")\n",
    "    for layer_index in range(n_layers):\n",
    "        # problem unitary\n",
    "        for comp in HP:\n",
    "            lstcomp = list(comp)\n",
    "            if len(lstcomp) == 3:\n",
    "                first_sign = 1\n",
    "            else:\n",
    "                first_sign = -1\n",
    "            #print(1<<(len(lstcomp)))\n",
    "            for mix in range(1, 1<<(len(lstcomp))):\n",
    "                newlst = []\n",
    "                for idx in range(0, len(lstcomp)):\n",
    "                    if (mix & (1<<idx)) != 0:\n",
    "                        newlst.append(lstcomp[idx])\n",
    "                #print(newlst)\n",
    "                if (len(newlst) % 2 != 0):\n",
    "                    sign = -first_sign\n",
    "                else:\n",
    "                    sign = first_sign\n",
    "                for it in range(0, len(newlst)-1):\n",
    "                    qc.cx(newlst[it], newlst[it+1])\n",
    "                qc.rz(2 * sign * gamma[layer_index], newlst[-1])\n",
    "                for it in range(len(newlst)-1, 0, -1):\n",
    "                    qc.cx(newlst[it-1], newlst[it])\n",
    "        for comp in HP_cons2:\n",
    "            lstcomp = list(comp[1])\n",
    "            first_sign = -comp[0]\n",
    "            #print(1<<(len(lstcomp)))\n",
    "            for mix in range(1, 1<<(len(lstcomp))):\n",
    "                newlst = []\n",
    "                for idx in range(0, len(lstcomp)):\n",
    "                    if (mix & (1<<idx)) != 0:\n",
    "                        newlst.append(lstcomp[idx])\n",
    "                #print(newlst)\n",
    "                if (len(newlst) % 2 != 0):\n",
    "                    sign = -first_sign\n",
    "                else:\n",
    "                    sign = first_sign\n",
    "                for it in range(0, len(newlst)-1):\n",
    "                    qc.cx(newlst[it], newlst[it+1])\n",
    "                qc.rz(2 * sign * gamma[layer_index], newlst[-1])\n",
    "                for it in range(len(newlst)-1, 0, -1):\n",
    "                    qc.cx(newlst[it-1], newlst[it])\n",
    "        # mixer unitary\n",
    "        for qubit in range(nqubits):\n",
    "            qc.rx(2 * beta[layer_index], qubit)\n",
    "    #print('End circuit')\n",
    "    qc.measure_all()\n",
    "    return qc\n",
    "\n",
    "\n",
    "def get_expectation(graph, HP, obj, shots=512):\n",
    "    backend = Aer.get_backend('qasm_simulator')\n",
    "    backend.shots = shots\n",
    "\n",
    "    def execute_circ(theta):\n",
    "        qc = create_qaoa_circ(graph, theta, HP)\n",
    "        counts = backend.run(qc, seed_simulator=10,\n",
    "                             shots=10000).result().get_counts()\n",
    "        return compute_expectation(counts, graph, obj)\n",
    "\n",
    "    return execute_circ\n",
    "\n",
    "def get_expectation_test(graph, HP, HP_cons2, obj, backend):\n",
    "\n",
    "    def execute_circ(theta):\n",
    "        qc = create_qaoa_circ(graph, theta, HP, HP_cons2)\n",
    "        counts = backend.run(qc, seed_simulator=10,\n",
    "                             shots=10000).result().get_counts()\n",
    "        #print(len(counts.keys()))\n",
    "        return compute_expectation(counts, graph, obj)\n",
    "\n",
    "    return execute_circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e507db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 322\n",
      "210 322\n",
      "218 316\n",
      "217 313\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "final_res = []\n",
    "P = nx.DiGraph()\n",
    "P.add_nodes_from([0, 1, 2])\n",
    "P.add_edges_from([(0, 1), (1, 2), (0, 2)])\n",
    "P[0][1]['weight'] = 0\n",
    "P[1][2]['weight'] = 1\n",
    "P[0][2]['weight'] = 0\n",
    "\n",
    "dataset_type = 'synthetic'\n",
    "motif_type = 'FFL_weighted'\n",
    "\n",
    "for t in range(1, 376, 1):\n",
    "    temp_res = {'test_case':t, 'edge_size': -1, 'nums_motifs':0, 'running_time': -1, 'saved_motifs': []}\n",
    "    edge_lst = []\n",
    "    #file_name = 'graph_generator/graph_'+str(nums_node)+'Nodes_'+str(degree)+'AvgDegree_'+str(graph_type)+'GraphType_'+str(i)+'_TestCase.txt'\n",
    "    file_name = 'dataset/'+dataset_type+'/'+motif_type+'/'+str(t)+'.txt'\n",
    "    weightG = {}\n",
    "    with open(file_name) as f:\n",
    "        while (True):\n",
    "            s = f.readline()\n",
    "            if s == '':\n",
    "                break\n",
    "\n",
    "            lst = list(s[:-1].split(\" \"))\n",
    "            #print(lst)\n",
    "            edge_lst.append((int(lst[0]), int(lst[1])))\n",
    "            #print(lst)\n",
    "            weightG[(int(lst[0]), int(lst[1]))] = int(lst[2])\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edge_lst)\n",
    "\n",
    "    G_new = G.copy()\n",
    "    res = 0\n",
    "    total_run_time = 0\n",
    "    total_saved_motifs = []\n",
    "    while True:\n",
    "        check_terminal = True\n",
    "        list_new = graph_partitioning(G_new, max_node = max(G.nodes)+1, cluster_limit = 25)\n",
    "        print(len(list_new), len(G_new.nodes))\n",
    "        #break\n",
    "        list_backend = []\n",
    "        for i in range(len(list_new)):\n",
    "            backend = Aer.get_backend('aer_simulator')\n",
    "            backend.shots = 512\n",
    "            list_backend.append(backend)\n",
    "\n",
    "        start = time.time()\n",
    "        package_res = Parallel(n_jobs=len(list_new))(delayed(mi_calculation_triangle)(list_new[i], P, weightG, list_backend[i]) for i in range(len(list_new)))\n",
    "\n",
    "        for p in package_res:\n",
    "            res_partition, removed_nodes, saved_motifs, run_time = p[0], p[1], p[2], p[3]\n",
    "            G_new.remove_nodes_from(removed_nodes)\n",
    "            if len(removed_nodes) != 0:\n",
    "                check_terminal = False\n",
    "            #print(res_partition, removed_nodes, saved_motifs, run_time)\n",
    "            res += res_partition\n",
    "            total_saved_motifs = total_saved_motifs + saved_motifs\n",
    "            if run_time != -1:\n",
    "                total_run_time += run_time\n",
    "        if check_terminal:\n",
    "            break\n",
    "\n",
    "    temp_res['nums_motifs'] = -res/10\n",
    "    temp_res['running_time'] = total_run_time\n",
    "    temp_res['saved_motifs'] = total_saved_motifs\n",
    "    temp_res['edge_size'] = len(edge_lst)\n",
    "    final_res.append(temp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6567c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results_file_name = 'dataset/'+dataset_type+'/'+motif_type+'/'+size+'/results.pkl'\n",
    "with open(results_file_name, 'wb') as f:\n",
    "    pickle.dump(final_res, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
